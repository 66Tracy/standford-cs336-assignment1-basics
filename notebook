1、Unicode标准
定义：是一个映射，从数字1~15w，每个Unicode值对应一个现代字符

2、UTF-8编码
定义：是一种将Unicode字符编码为字节序列的方式，有的字符使用1个字节表示，有的字符可能需要2、3、4个字节表示
例如：
'h': 对应Unicode值104，UTF-8编码为1个字节：01101000（0x68）
'こ': 对应Unicode值12371，UTF-8编码为3个字节：11100011 10000010 10101111（0xE3 0x81 0x93），这里的0xE3占据第一个字节，\0xe3和\0xe4通常是3字节序列的起始字节，表示中文字符或某些其他非ASCII字符。它们本身不代表一个完整的字符，而是构成一个多字节序列的第一部分，用于标识该字符的Unicode码点属于特定范围。 
'你'.encode("utf-8") = b'\xe4\xbd\xa0'
* 所以assignment1中的decode_utf8_bytes_to_str_wrong函数一定失败，因为它试图将每个字节单独解码，而遇到\0xe3会直接报错，因为它不是一个完整的字符

3、byte-level Tokenization
如果基于UTF-8编码后的字节序列用于训练，那么原本10个单词（假设等价10个token），转换为字节后，可能变成30个字节（假设等价30个token），这样会大大增加序列长度，影响模型训练效率

4、SubWord Tokenization -> BPE
2016年，Sennrich提出BPE，即，如果the这样的单词经常出现，则考虑the当做一个token，而不是t-h-e三个token

5、Pre-Tokenization
如果直接从byte级别开始做遍历+合并，那么很可能出现"dog!"和"dog."这样两个意义近似，但却要占据两个不同token id的情况，屡见不鲜
所以，在正式BPE合并之前，会先进性pre-tokenization，即粗粒度地先进行分词。比如统计"text"单词数量频次最高，那么，基于预分词的情况，再统计t\e字母相邻的情况。
GPT-2使用了如下的分词方式：
>>> PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
>>> # requires `regex` package
>>> import regex as re
>>> re.findall(PAT, "some text that i'll pre-tokenize")
['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']

6、BPE合并
1）经过Pre-Tokenization后，将各个词元encoding为utf-8编码；
2）基于1）的utf-8编码进行统计，再合并高频
3）BPE有一个规则是，当有多个合并对象频次一致，则按字母表序更大的先合并
*** 什么是“字典序”？***
关键背景：什么是 “字典序（lexicographical order）”？
字典序是一种模仿字典查词逻辑的排序规则，核心是按字符在字符集中的编码顺序（如 ASCII 码）逐位比较，而非按字符串长度或语义。
比较规则：
从两个字符串的第一个字符开始逐位对比；
若某一位字符不同，编码更大的字符所在的字符串整体更大；
若前面所有字符都相同，更长的字符串更大；
若长度和所有字符都相同，则两字符串相等（无 “更大” 关系）。
一个例子：基于 ASCII 码：数字 < 大写字母 < 小写字母，同类型按顺序
******

7、BPE加速
1）第一个阻塞点：Pre-Tokenization
使用并行加速，注意，要先标记并移除所有special token的位置，以免被切分
2）第二个阻塞点：合并
如果真的每次合并后都重新遍历并统计词表，其实有大量的重复冗余计算；实际上，发生改变的只有被合并的那个token及其相邻token，所以，可以只更新这些token的统计频次，从而大幅提升效率。
** BPE过程在python无法并行化 **

8、